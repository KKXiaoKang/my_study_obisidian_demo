![[Pasted image 20250616150958.png]]
Soft Actor-Critic（SAC）是一种基于最大熵强化学习的off-policy算法，核心目标是在获得高回报的同时，最大化策略的熵（即探索性）。下面结合你给出的伪代码，详细解析其算法流程和核心逻辑：
![[Pasted image 20250619104646.png]]
### SAC loss 函数
## 🧠

$$
\text{target}_Q = r + \gamma \cdot (1 - \text{done}) \cdot \left[ \min_i Q_{\text{target}}(s', a') - \alpha \cdot \log \pi(a'|s') \right]
$$
* 这和 SAC 的核心思想一致：**最大化预期回报 + 保持策略多样性（熵项）**。

### SAC train流程如下
---
### 1. **初始化参数**
- 初始化策略网络参数 $\phi$、两个Q函数网络参数 $\theta_1, \theta_2$、目标Q网络参数 $\bar{\psi}$ 以及主Q网络参数$\psi$。
### 2. **主循环（for each iteration）**
每次迭代分为两个阶段：**与环境交互** 和 **参数更新**。
#### **A. 与环境交互（for each environment step）**
- **动作采样**：根据当前策略 $\pi_\phi$，给定当前状态 $s_t$，采样动作 $a_t$。
- **环境反馈**：将动作 $a_t$ 作用于环境，获得下一个状态 $s_{t+1}$ 和奖励 $r(s_t, a_t)$。
- **经验存储**：将 $(s_t, a_t, r(s_t, a_t), s_{t+1})$ 存入经验回放池 $\mathcal{D}$。
**核心逻辑**：  
Agent通过策略网络（通常是高斯策略）对当前状态采样动作，和环境交互，收集经验数据，形成经验池。
#### **B. 参数更新（for each gradient step）**
- **Q网络更新**：用经验池中的样本，最小化Q网络的损失，更新Q网络参数 $\theta_1, \theta_2$。
- **策略网络更新**：最大化策略的期望回报和熵，更新策略参数 $\phi$。
- **目标Q网络软更新**：用软更新方式$\tau$ 很小更新目标Q网络参数 $\bar{\psi}$。
**核心逻辑**：  
- Q网络学习当前策略下的动作价值，目标是拟合贝尔曼方程。
- 策略网络不仅追求高Q值，还鼓励高熵（即多样性/探索性）。
- 目标Q网络用于稳定训练，防止Q值估计发散。
### 3. **Agent与环境的交互与价值估计**
- **观测环境**：Agent通过状态 $s_t$ 观测环境。
- **动作选择**：策略网络 $\pi_\phi$ 根据 $s_t 采样动作 $a_t$。
- **环境反馈**：环境返回 $s_{t+1}$ 和奖励 $r(s_t, a_t)$。
- **价值估计**：Q网络根据 $(s_t, a_t)$ 估计动作价值，目标Q网络用于计算目标值。
- **策略改进**：策略网络根据Q网络的反馈和熵奖励进行更新。
### 4. **核心逻辑总结**
- **最大熵目标**：不仅追求高回报，还鼓励策略的随机性（探索性）。
- **off-policy**：利用经验回放池，提升样本利用率。
- **双Q网络**：减少Q值高估偏差。
- **软更新目标网络**：提升训练稳定性。
