
### 核心摘要
* 主要idea : 阶段1和阶段2都在学一个精细化操作，学从一个大概的很近位置去插插座，阶段3通过VLM来精确定位，从而提高空间上的初始泛化能力（具体阶段3的数据流理解还得重新思考）
	* 阶段 1 ：语义增强的场景数据生成（真机图像+数据合成）
		* 原图像上选取关键点key-point(关键点仅用于`Vision Foundation Model`抠图)
		* 分割图像(`Segmented Image`)+程序生成的各种背景
		* 数据流 (A): 原始图像 → 人工标注关键点 → 物体分割 → 程序化背景替换 → 大规模合成数据集
	* 阶段2 ： 训练策略阶段
		* 离线阶段：通过上述的数据训练离线训练通过BC生成一个策略$\pi^{b}$ 
			* 输入: 上一阶段 (A) 生成的大规模合成数据集
			* 过程: 使用模仿学习（或行为克隆）算法，让机器人学习一个“基础策略” $\pi^{b}$ 。这个策略通过“看”这些合成数据，学会任务的大致流程和基本动作
			* 产出: 一个能完成任务但可能不够精准的基础策略 $\pi^{b}$ 
		* 在线阶段 ：残差强化学习 (Phase 2: Residual RL)
			* 过程: 机器人在真实（或高仿真）环境中，开始执行基础策略 $\pi^{b}$ 。同时，一个“残差策略”$\pi^{r}$  开始在线学习
			* 核心思想: $\pi^{r}$ 不从头学习整个动作，它只学习对基础策略 $\pi^{b}$  的微小修正或补偿。最终执行的动作是 基础动作 + 残差修正。这使得学习过程更稳定、更高效
			* 产出: 一个专门进行精细调整的残差策略 $\pi^{r}$ 
		* 数据流 (B): 合成数据集 → 训练基础策略 $\pi^{b}$ → (在线部署 $\pi^{b}$) + (在线学习残差策略 $\pi^{r}$ ) → 最终高精度策略 ($\pi^{b}$ + $\pi^{r}$ ) 
	* 阶段3 ：通过VLM实现空间泛化 (Spatial Generalization through VLM Reaching)
		* 面临问题: (B)阶段训练的策略是在一个固定的物体位置上学习的。如果把咖啡机或者插座换个地方，策略就失效了
		* 解决方案 (Molmo / VLM): 引入一个强大的视觉语言模型 (Vision-Language Model, VLM
		* 工作流程:
			- 输入: 一张新场景的图片 + 一句自然语言指令 (例如: "Mark a point on the socket" / “在插座上标记一个点”)。
			- VLM处理: VLM能够理解语言指令，并在图像中找到对应的物体（“插座”），然后定位出精确的目标点。
			- 产出: 目标在当前图像中的精确坐标。
		* 最终执行: 将VLM输出的坐标作为目标，(B)阶段训练好的高精度策略随即在该坐标上执行其已经学会的精细操作（例如插入插头）。
		* 数据流 (C): 新场景图像 + 语言指令 → VLM理解并定位 → 输出目标坐标 → 策略在新坐标上执行任务
* 核心pipeline
![[Pasted image 20250707113453.png]]
  * 这是一个非常典型的现代机器人学习流程，旨在用最少的人力（只需标注几个点）来训练一个能适应多变环境（视觉和空间变化）的、高精度的机器人策略。

---

### 工作流总览

**核心思想：** 先用“免费”的合成数据进行粗略的模仿学习，再用真实的在线交互进行精细的强化学习，最后用强大的视觉语言模型（VLM）来解决目标位置变化的问题。

---

### (A) 阶段一：为场景泛化进行语义增强 (Semantic Augmentation)

这个阶段的目标是：**用最小的代价，创造出海量的、多样化的训练数据，让机器人策略学会忽略无关的背景变化，专注于任务本身。**

1.  **输入源 (`Wrist Image`):** 从机器人手腕上的相机获取一张包含操作对象（这里是咖啡机）的清晰图像，背景很简单（绿色幕布）。
2.  **人工标注 (`Sparse Human Annotations`):** 人类操作员在物体上标注几个关键点（图中的红点）。这是一个**一次性**的、非常低成本的步骤。
3.  **分割与追踪 (`Segment and Track`):** 基于这几个关键点，一个视觉模型（如图中描述的 Vision Foundation Model）会自动地将物体从背景中“抠”出来，生成一个只包含物体的“分割图像”（`Segmented Image`）。
4.  **数据合成 (`RoboEngine`):** 这是一个程序化生成引擎。它会将上一步“抠”出来的物体，粘贴到成千上万个随机生成的、五花八门的背景上。
5.  **最终产出:** 海量的、带有不同背景的合成图像数据。

**数据流 (A):**
`原始图像` → `人工标注关键点` → `物体分割` → `程序化背景替换` → **`大规模合成数据集`**

---

### (B) 阶段二：视觉-触觉残差强化学习 (Visuo-Tactile Residual RL)

这个阶段的目标是：**训练出一个高精度的机器人操作策略。** 它分为两个子阶段：

1.  **第一阶段：离线模仿 (`Phase 1: Offline Imitation`)**
    *   **输入:** 上一阶段 (A) 生成的大规模合成数据集。
    *   **过程:** 使用模仿学习（或行为克隆）算法，让机器人学习一个“基础策略” `π^b`。这个策略通过“看”这些合成数据，学会任务的大致流程和基本动作。
    *   **产出:** 一个能完成任务但可能不够精准的**基础策略 `π^b`**。

2.  **第二阶段：残差强化学习 (`Phase 2: Residual RL`)**
    *   **过程:** 机器人在真实（或高仿真）环境中，开始执行基础策略 `π^b`。同时，一个“残差策略”`π^r` 开始在线学习。
    *   **核心思想:** `π^r` 不从头学习整个动作，它只学习对基础策略 `π^b` 的**微小修正或补偿**。最终执行的动作是 `基础动作 + 残差修正`。这使得学习过程更稳定、更高效。
    *   **产出:** 一个专门进行精细调整的**残差策略 `π^r`**。

**数据流 (B):**
`合成数据集` → `训练基础策略 π^b` → `(在线部署 π^b) + (在线学习残差策略 π^r)` → **`最终高精度策略 (π^b + π^r)`**

---

### (C) 阶段三：通过VLM实现空间泛化 (Spatial Generalization through VLM Reaching)

这个阶段的目标是：**让训练好的策略能够适应目标物体位置的变化，实现“指哪打哪”。**

1.  **面临问题:** (B)阶段训练的策略是在一个固定的物体位置上学习的。如果把咖啡机或者插座换个地方，策略就失效了。
2.  **解决方案 (`Molmo` / VLM):** 引入一个强大的视觉语言模型 (Vision-Language Model, VLM)。
3.  **工作流程:**
    *   **输入:** 一张新场景的图片 + 一句自然语言指令 (例如: "Mark a point on the socket" / “在插座上标记一个点”)。
    *   **VLM处理:** VLM能够理解语言指令，并在图像中找到对应的物体（“插座”），然后定位出精确的目标点。
    *   **产出:** 目标在当前图像中的**精确坐标**。
4.  **最终执行:** 将VLM输出的坐标作为目标，(B)阶段训练好的高精度策略随即在该坐标上执行其已经学会的精细操作（例如插入插头）。

**数据流 (C):**
`新场景图像 + 语言指令` → `VLM理解并定位` → `输出目标坐标` → **`策略在新坐标上执行任务`**

### 总结

ViTAL 的完整工作流可以概括为：

1.  **数据准备（离线）:** 通过“抠图+换背景”的方式，低成本地生成海量视觉训练数据。
2.  **策略训练（离线+在线）:** 先用合成数据模仿学习一个基础策略，再通过在线残差强化学习对策略进行“精雕细琢”，达到高精度。
3.  **智能部署（在线）:** 在全新的、目标位置发生变化的场景中，利用VLM的“眼睛和大脑”来定位目标，然后调用训练好的精准策略来完成任务。