## `tain` è¿‡ç¨‹è¯¦è§£ 
#### 1) ä»bufferä¸­é‡‡æ ·å†å²æ•°æ®
```python
obs, actions, rewards, next_obs, dones = self.replay_buffer.sample(self.batch_size)

...
def sample(self, batch_size):
    idx = random.sample(range(self.size), batch_size)
    return (
        self.obs[idx],
        self.actions[idx],
        self.rewards[idx],
        self.next_obs[idx],
        self.dones[idx]
    )

```
* `random` ä»bufferå½“ä¸­ä¸æ”¾å›çš„å‡åŒ€éšæœºé‡‡æ ·`batch_size`ä¸ªç´¢å¼•,  ä¸€èˆ¬è®¾ç½®çš„`batch_size`ä¸º128
* éšæœºæŠ½å–`128ä¸ªidx` ï¼ŒèŒƒå›´ä»`0åˆ° buffer_size`


#### 2) å½’ä¸€åŒ–è§‚æµ‹
```python
# å½’ä¸€åŒ–è§‚æµ‹
## ç´¯è®¡å¾—åˆ°çš„æ»‘åŠ¨å‡å€¼å’Œæ–¹å·®
mean = torch.as_tensor(self.obs_rms.mean, dtype=torch.float32, device=self.device)
var = torch.as_tensor(self.obs_rms.var, dtype=torch.float32, device=self.device)
# ç¡®ä¿varéè´Ÿ
var = torch.clamp(var, min=1e-8) # ç¡®ä¿var â‰¥ 1e-8
## é€šè¿‡å‡å»å‡å€¼å°†åˆ†å¸ƒä¸­å¿ƒåŒ–ï¼Œé€šè¿‡é™¤ä»¥æ ‡å‡†å·®ï¼ˆå³ sqrt æ–¹å·®ï¼‰å°†å…¶ç¼©æ”¾åˆ°å•ä½æ–¹å·®
normalized_obs = (obs - mean) / torch.sqrt(var)
normalized_next_obs = (next_obs - mean) / torch.sqrt(var)
# è£å‰ªè§‚æµ‹å€¼ è£å‰ªä»¥é¿å…æç«¯æ•°å€¼
clipped_obs = torch.clamp(normalized_obs, -10.0, 10.0)
clipped_next_obs = torch.clamp(normalized_next_obs, -10.0, 10.0)
```
```bash
obs = torch.tensor([11.0, 18.0, 30.0])
next_obs = torch.tensor([12.0, 20.0, 33.0])
mean = [10.0, 15.0, 25.0]  # å‡å€¼
var = [4.0, 9.0, 25.0]

normalized_obs = (obs - mean) / torch.sqrt(var) 
               = ([11, 18, 30] - [10, 15, 25]) / [2, 3, 5]
               = [0.5, 1.0, 1.0]

normalized_next_obs = (next_obs - mean) / torch.sqrt(var)
					= ([12, 20, 33] - [10, 15, 25]) / [2, 3, 5]
                    â‰ˆ [1.0, 1.667, 1.6]

# è£å‰ªåˆ°-10åˆ°10ï¼Œå…¶å®æ²¡æœ‰å˜æ¢
clipped_obs = torch.clamp(normalized_obs, -10.0, 10.0)
clipped_next_obs = torch.clamp(normalized_next_obs, -10.0, 10.0)
```

#### 3) æ›´æ–°Criticè¯„è®ºå®¶
* ***calculate_loss_q** å‡½æ•°å°±æ˜¯è®¡ç®—Qå€¼æŸå¤±(åŒç›®æ ‡Qç½‘ç»œï¼Œé˜²æ­¢è¿‡é«˜ä¼°è®¡)
    *  `æœ€å¤§åŒ–é¢„æœŸå›æŠ¥ + ä¿æŒç­–ç•¥å¤šæ ·æ€§ï¼ˆç†µé¡¹ï¼‰`
    * `ç›®æ ‡Qå€¼å‡½æ•°å…¬å¼å¦‚ä¸‹`
$$
\text{target}_Q = r + \gamma \cdot (1 - \text{done}) \cdot \left[ \min_i Q_{\text{target}}(s', a') - \alpha \cdot \log \pi(a'|s') \right]
$$
* ä»»æ„`å‡½æ•°Qçš„æŸå¤±å‡½æ•°`å®šä¹‰å¦‚ä¸‹
![[Pasted image 20250618183018.png]]
```python
# è®¡ç®—Qå€¼æŸå¤±
q_loss = self.policy.calculate_loss_q(clipped_obs, actions, rewards, clipped_next_obs, dones, self.gamma)

# è¯„è®ºå®¶
self.policy.critic_optimizer.zero_grad()

# åå‘ä¼ æ’­
q_loss.backward()

# è¯„è®ºå®¶step
self.policy.critic_optimizer.step()
```
```python
def calculate_loss_q(self, obs, actions, rewards, next_obs, dones, gamma):
"""
è®¡ç®—Qå€¼æŸå¤±
:param obs: å½“å‰çŠ¶æ€
:param actions: å½“å‰åŠ¨ä½œ
:param rewards: å¥–åŠ±
:param next_obs: ä¸‹ä¸€ä¸ªçŠ¶æ€
:param dones: æ˜¯å¦ç»“æŸ
:param gamma: æŠ˜æ‰£å› å­
"""
	# ç›®æ ‡ç½‘ç»œæ— æ¢¯åº¦è®¡ç®—Qå€¼
	with torch.no_grad():
		next_actions, log_pi_next, _ = self.actor(next_obs)
		target_q_values = self.critic_target(next_obs, next_actions)
		target_q_min = target_q_values.min(1)[0]
		target_q = rewards + (1 - dones) * gamma * (target_q_min - \
		           self.get_alpha().detach() * log_pi_next)
	# å½“å‰ç½‘ç»œè®¡ç®—Qå€¼
	current_q = self.critic(obs, actions) # [batch_size, n_critics]
	# è®¡ç®—Qå€¼æŸå¤±
	q_loss = 0.5 * (current_q - target_q.unsqueeze(1)).pow(2).sum(dim=1).mean()
	return q_loss
```


#### 4) æ›´æ–°ç­–ç•¥ç½‘ç»œActor
* ç­–ç•¥å‡½æ•°çš„æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š

$$
\mathcal{L}_\pi = \mathbb{E}_{s \sim D,\, a \sim \pi(\cdot|s)} \left[ \alpha \cdot \log \pi(a|s) - Q(s, a) + \text{action penalty} \right]
$$
```python
alpha_loss = self.policy.calculate_loss_alpha(log_pi)
self.policy.alpha_optimizer.zero_grad()
alpha_loss.backward()
self.policy.alpha_optimizer.step()
```
```python
def calculate_loss_pi(self, obs):
	"""
	è®¡ç®—ç­–ç•¥æŸå¤±
	:param obs: å½“å‰çŠ¶æ€
	"""
	actions_pi, log_pi, action_penalty = self.actor(obs) # å½“å‰ç½‘ç»œè®¡ç®—åŠ¨ä½œ
	q_values_pi = self.critic(obs, actions_pi)
	min_qf_pi = q_values_pi.min(1)[0]
	policy_loss = (self.get_alpha().detach() * log_pi - min_qf_pi + action_penalty).mean()
	
	return policy_loss, log_pi
```

#### 5) æ›´æ–°alpha - æ›´æ–°ç†µå‚æ•°
```python
def calculate_loss_alpha(self, log_pi):
	alpha_loss = (-self.log_alpha * (log_pi + self.target_entropy).detach()).mean()
	return alpha_loss
```
ğŸ§  å¯¹åº”çš„æ•°å­¦å…¬å¼
è¿™ä¸ªæŸå¤±å‡½æ•°å¯¹åº”çš„æ˜¯ä¸‹é¢è¿™ä¸ªç›®æ ‡çš„è´Ÿæ¢¯åº¦æ–¹å‘ï¼š
$$
\mathcal{L}_\alpha = \mathbb{E}_{a \sim \pi} \left[ \alpha \cdot \left( -\log \pi(a|s) - \mathcal{H}_{\text{target}} \right) \right]
$$
ä»¤ $\alpha = \exp(\log \alpha)$ï¼Œå®é™…ä¸Šä¼˜åŒ–çš„æ˜¯ï¼š
$$
\mathcal{L}_{\log \alpha} = -\log \alpha \cdot \left( \log \pi(a|s) + \mathcal{H}_{\text{target}} \right)
$$
#### 6) æ›´æ–°ç›®æ ‡ç½‘ç»œ
```python
def update_target_network(self, tau=0.005):
	# è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
	for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
		target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
```
*ï¼ˆå¦‚Â Î¸â€²â†Ï„Î¸+(1âˆ’Ï„)Î¸â€²Î¸â€²â†Ï„Î¸+(1âˆ’Ï„)Î¸â€²ï¼‰