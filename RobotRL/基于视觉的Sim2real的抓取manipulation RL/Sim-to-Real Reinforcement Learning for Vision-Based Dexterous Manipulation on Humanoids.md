* https://arxiv.org/abs/2502.20396
* [https://toruowo.github.io/recipe](https://toruowo.github.io/recipe)
* ![[Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids.pdf]]
###  论文核心 - pipeline

![[Pasted image 20250806145311.png]]

#### B.通用的奖励设计方案 - 用于操作任务抓取
![[Pasted image 20250806170312.png]]
* manipulation的dense reward 参考设计
* 对于灵巧手抓取任务（长时程-接触丰富的操作任务），定义动作序列状态
	* 手 - 物体接触 - 物体状态组合
*  任务状态 - 根据接触状态和物体状态的交错序列
	* 1） 一只手接触物体
	* 2）物体被抬起到另一只手附近的位置
	* 3）另一只手接触物体
	* 4）物体被转移到最终目标位置
* 奖励仅基于“接触目标”和“物体目标”来定义
	*  每个接触目标可以通过惩罚手指到理想接触点或物体质心位置的距离来指定
	*  每个物体目标可以通过惩罚其当前状态到其Target目标状态的距离来指定（distance reward）
* 奖励公式 物体接触公式
	* 机器人要操作一个物体，就得跟物体**产生接触**。但具体**接触哪里、如何接触**，是学习的关键。为了让学习过程知道**哪些接触是“好”的**，作者设计了一个**基于“接触目标点”的奖励函数**
$$r_{contact} = \sum_i \left[ \frac{1}{1 + \alpha d(\mathbf{X}_i^L, \mathbf{F}_i^L)} + \frac{1}{1 + \beta d(\mathbf{X}_i^R, \mathbf{F}_i^R)} \right]$$
* 其中每一项的含义如下：
	* $\mathbf{X}_i^L$ , $\mathbf{X}_i^R$ 
		* 在第$i$个任务中， 左/右手的目标接触点坐标（你希望它解除的物体上的点）
	* $\mathbf{F}_i^L$ , $\mathbf{F}_i^R$
		* 实际左/右手指尖的接触点坐标（实际当前接触位置）
	* $d(\mathbf{A}, \mathbf{B})$ 
		* 点之间的欧式距离 $min_i||A_i - X_i||_2$  
	*  $\alpha, \beta$ 
		* 缩放参数，调节距离误差在reward中的影响


#### A.自动从真实到模拟调优的模块
![[Pasted image 20250806170301.png]]
##### Autotuned robot modeling. - 自动调优机器人模型
* 模块使模拟环境更接近真实世界， 自动调优的机器人建模
* 该模块由两个部分构成
	* simEnv：影响运动学和动力学的模拟器物理参数 （damping stiffness）
	* realEnv：来自URDF文件的机器人模型常数（包含连杆惯性值、关节限制和关节/连杆姿态）
* 校准过程
	* 1）使用从参数空间中随机采样的参数组合初始化多个模拟环境, 参数组合来源于机器人urdf文件
	* 2）并行执行$N$个校准序列，包含在真实机器人硬件（单次）运行和所有模拟环境中设置的目标关节位置
	* 3）对比每个模拟环境在遵循相同关节目标时与真实机器人的之间的跟踪误差，该模块选择在跟踪性能中均方误差最小的参数集
* 参数包含
	* simEnv：damping  | stiffness | armature | effort
	* realEnv：惯性矩阵值
##### Approximate object modeling
* 近似物体建模。 论文最早提出参考如下
	* https://arxiv.org/abs/2403.02338
	* https://arxiv.org/abs/2210.04887
* 将物体建模为具有随机参数的圆柱等基本形状，足以让基于视觉的人形机器人灵巧操作策略实现仿真到现实的迁移。我们的方法采用了这种策略，并发现其效果显著
#### C. Sample Efficient Policy Learning
![[Pasted image 20250806170252.png]]
* 一个分而治之的蒸馏过程 | 该过程提高了硬探索问题的样本效率
* 由于在高维空间探索中存在样本复杂性和奖励稀疏性
	* 尤其在具有多指手的人形机器人平台上 
	* 即使有明确的奖励函数，策略学习也可能需要过长的时间
* 文章提出两种提高策略学习样本效率的奇数
	* 1）使用任务感知的手部姿态初始化任务
	* 2）将一个具有挑战性的任务分解为更容易的子任务，然后将子任务专家提炼为通用策略

 * 1） 使用任务感知的手部姿态初始化任务 - 通过玩耍来采集每个episode的初始状态分布
	 *  遥操作 *play aroud* ：收集的状态state，包括物体姿态pose和机器人关节位置joint angle，然后作为任务初始化状态在模拟中进行随机采样。随机进行玩耍，玩耍当中的某一个frame会作为每个episode的初始状态
		 * 将这些状态**作为每个 episode 的初始状态**，用来 reset 环境
		 * 然后从这个状态出发，开始 PPO 的采样和学习
* 2） 专家蒸馏为通用策略
	* 多物体操作任务可以分解为多个单物体操作任务
	* 在将复杂任务分解为更简单的子任务后，我们可以为每个子任务训练专门的策略，并将它们蒸馏为通用策略
	* 这种方法的一个好处是我们可以根据子任务策略的优化程度灵活地过滤掉轨迹数据，仅保留高质量的样本用于训练
	* 这有效地使强化学习更接近于从演示中学习，其中子任务策略在模拟环境中充当任务数据收集的远程操作员，而通才策略则充当从精选数据中训练的集中式模型

#### D.稀疏和密集物体表示的混合 | Vision-Based Sim-to-Real Transfer
* 如何解决sim2real policy的视觉gap和物体接触（动力学）gap？

##### Mixing object representations. |  混合物体表示