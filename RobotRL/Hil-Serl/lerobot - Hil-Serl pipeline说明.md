![[Pasted image 20250703140939.png]]
## 关于SAC当中离散Q值网络的实现指南
### 机器人控制的实际需求
#### 连续动作：
- 关节速度控制
- 末端执行器位置控制
- 力控制
#### 离散动作：
- 夹爪开关控制
- 工具选择
- 操作模式切换
- 任务阶段选择

## 关于SAC当中的Buffer经验回放池的设计pipeline及区别
### 两种回放缓冲区的区别

| 特性   | replay_buffer (在线)                             | offline_replay_buffer (离线)                                            |
| ---- | ---------------------------------------------- | --------------------------------------------------------------------- |
| 数据来源 | Actor实时收集的环境交互数据                               | 预定义的数据集（如人类演示数据）                                                      |
| 更新方式 | 动态添加新的转换数据<br><br>数据构成：policy探索的数据 + 人工在线干预的数据 | 静态加载，训练过程中正常不更新,但是有一些方法除外，比如Hil-Serl当中<br><br>数据构成：专家演示数据 + 人工在线干预的数据 |
| 容量   | cfg.policy.online_buffer_capacity              | cfg.policy.offline_buffer_capacity                                    |
| 用途   | 存储在线探索的经验                                      | 存储高质量的人类演示数据                                                          |
|      |                                                |                                                                       |
