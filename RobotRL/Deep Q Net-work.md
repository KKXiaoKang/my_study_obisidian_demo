### Q-learning 讲解
* 这是一个[Flappy Bird](https://zhida.zhihu.com/search?content_id=120377040&content_type=Article&match_order=1&q=Flappy+Bird&zhida_source=entity)小游戏
![[Pasted image 20250616153000.png]]
* **状态State**：将每一帧作为一个状态，取小鸟离下一个地面上柱子在水平和竖直方向上的距离作为状态的观测值，即下图中的(△x, △y)；
- **行为Action**：对每一个状态（每一帧），只有两种选择：跳，不跳；
- **奖励Reward**：小鸟活着时给每帧奖励1，死亡时奖励-1000。
  在该游戏中，程序是如何选择该跳还是不该跳呢？按照前面说的Q学习算法，那么它应该是需要有一个`Q(S, A)`函数的，可以知道在什么状态时采取什么样的行为能得到最大的Reward之和。在这个游戏中，很显然状态和动作的组合都是有限的，因此可以维护一个`S-A`表，其记录了在每个状态下，采用什么动作时能得到什么样的`Q`值。表格形式如下，只要程序在运行中不断更新这个表格，使其最终能收敛，那么程序就能拿得到的`state`通过查表的方式来判断它该选择什么样的行为，才能获得最大的Q值。
![[Pasted image 20250616153015.png]]
### Q-值更新
**Q-值更新方法**  
Q值大体上有两种更新方式，一种是类似上面`小鸟游戏`例子中的情况，状态和行为的组合是可以穷尽的情况，这时候往往采用的是`S-A`表格的形式记录Q值，而如果状态和行为的组合不可穷尽，比如自动驾驶中输入的外界环境照片与车速之间的组合是有无穷种的，那么前一种方法显然就不适用了，这时候常用的方式为将深度学习与Q学习结合起来，也就是本文的重点，DQN，这个我们将在后面重点讲解。
你的问题非常好，下面详细解释Q-Learning在初始化所有状态-动作对（S-A对）的Q值为0时，是如何进行更新和优化的。

1. **探索（Exploration）**  
   - Agent通常采用ε-greedy策略：以概率ε随机选择动作（探索），以概率1-ε选择当前Q值最大的动作（利用）。
   - 由于Q值都为0，初期主要靠探索。

2. **获得奖励并更新Q值**  
   - Agent执行动作 \( a_t \)，获得奖励 \( r_t \)，到达新状态 \( s_{t+1} \)。
   - 用上面的Q-Learning公式更新 \( Q(s_t, a_t) \)。
   - 由于初始Q值为0，第一次更新后，Q值就会变成 \( \alpha r_t \)（如果未来奖励为0）。

3. **逐步逼近最优Q值**  
   - 随着与环境的不断交互，Agent会不断采集新的（s, a, r, s'）四元组。
   - 每次更新都会让Q值更接近真实的期望回报。
   - 只要Agent不断探索，所有S-A对的Q值都会被访问并更新。。

如需代码示例或更深入的数学推导，可以继续提问！
##### S-A表格如何更新？

![[Pasted image 20250616154526.png]]
* `s`为状态State，`a`为采取的行为Action，`α`参数用来表示新的值对更新后值所造成的影响大小，`r`为在状态`s`下采取动作`a`后获得的奖励Reward，`γ`也是一个discount值，即用来减小新值的影响的值。其中`α`和`γ`的范围都在`0~1`之间

##### 策略Policy选择注意事项  
前面讲了，在强化学习中最重要的部分就是策略的选择，`S-A`表格说白了也不过是给选择哪个策略提供了一个参考。而在实际实验中，如果对每个状态`s`值，都选择其能获得最大Q值的行为去执行，是有问题的！  
假使初始的S-A表格所有值全为0，那么在状态s采用随机一个行为（比如a1），并第一次获得reward后，如果reward值大于0，那么以后再遇见状态s时，程序都会直接采用行为a1，然而，还有很多种没有行为从来都没有常识过，说不定采取其他的行为会使得它能得到的Q值更大。  
因此在强化学习中，往往需要设置一个阈值`ε`来保持一定的随机程度，即在每次做决定前，先生成一个随机数，如果这个随机数比`ε`小，那么就随机选取一个action，否则才选取当前已知条件下能使得Q值最大的action。这个阈值`ε`往往一开始被设置地很大，而其值也会随着程序不断地迭代而慢慢衰减，一般也需要给其设置一个最小值，即衰减到最小值后就停止衰减了。这样的好处是使得程序可以遍历所有的`S-A`对，以准确判断在给定状态下选择哪个行为最优，而这种做法被称为**exploration**，这种算法叫做**e-greddy**。

### DQN -- pipeline
![[Pasted image 20250616161940.png]]
* DQN属于DRL（深度强化学习）的一种，它是深度学习与Q学习的结合体。前面讲了采用`S-A`表格的局限性，当状态和行为的组合不可穷尽时，就无法通过查表的方式选取最优的Action了。这时候就该想到深度学习了，想通过深度学习找到最优解在很多情况下确实不太靠谱，但是找到一个无限逼近最优解的次优解，倒是没有问题的。  
因此DQN实际上，总体思路还是用的Q学习的思路，不过对于给定状态选取哪个动作所能得到的Q值，却是由一个深度神经网络来计算的了，其流程图如下：
##### DNN如何训练
* 现在我们的选择哪个动作，是由DNN来做决定的，因此我们需要训练DNN以使其能达到令人满意的表现，这显然是一个监督学习的问题，那么训练集是什么，标签是什么，损失函数又是什么？  

首先，我们DNN的输出值，自然是在给定状态的情况下，执行各action后能得到的Q值。然而事实上我们在很多情况下并不知道最优的Q值是什么，比如自动驾驶、围棋等情况，所以似乎我们没法给出标签。但是什么是不变的呢？**Reward**！

对状态s，执行动作a，`那么得到的reward是一定的，而且是不变的！`  因此需要考虑从reward下手，让预测Q值和真实Q值的比较问题转换成让模型实质上在拟合reward的问题
  
  
### DQN -- 代码构建
Q网络(Deep Q-Network, DQN)是深度强化学习中的一个重要概念，让我来详细解释一下：

1. 基本概念：
- Q网络是一个深度神经网络，用于近似Q函数(Q-function)
- Q函数表示在状态s下采取动作a的预期累积奖励
-  在状态`S`下，采取动作`A`后，**未来**将得到的奖励`Reward`值之和

2. 主要特点：
- 使用神经网络替代传统的Q表格
- 可以处理连续或高维的状态空间
- 能够泛化到未见过的状态

3. 网络结构：
```python
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

4. 训练过程：
- 使用经验回放(Experience Replay)存储转换样本
- 使用目标网络(Target Network)稳定训练
- 通过最小化TD误差来更新网络参数

5. 主要优势：
- 可以处理高维输入
- 不需要显式存储所有状态-动作对
- 能够学习复杂的策略

4. 改进版本：
- Double DQN
- Dueling DQN
- Prioritized Experience Replay
- Noisy Networks