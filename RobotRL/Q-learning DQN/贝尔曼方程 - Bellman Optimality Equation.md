* 动态规划方程构建方式
### 状态值函数 $V(s)$
* target
	* 回答的是 “处于状态 s 有多好？”
![[Pasted image 20250707151726.png]]
$$V_{\pi}(s) = \sum_a \pi(a | s) \cdot q_{\pi}(s, a)$$
* 其中$q_{\pi}(s, a)$ 是指该状态$s$下选取动作$a$的Q值
* $\pi(a | s)$ 则是该状态$s$下选取$a$动作的可能性
* 计算 ： 而$\sum_{a}$ 就是将上述的该状态$s$下所有动作的Q值求一个加权平均值, 权重来源于$\pi(a | s)$ 

> $V(s)$ 是对未来所有可能性的一种`综合期望`。 假设 “你在某个十字路口（状态s）的前景如何（V值）”，取决于“你可能选择的各条道路（动作a）的前景（Q值）”以及“你选择走每条路的概率（策略π）”


### 动作值函数 $Q(s, a)$ 
* target
	* “在状态 s 下，执行动作 a 有多好？”
![[Pasted image 20250707152859.png]]
$$q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a)\cdot[r+\gamma v_{\pi}(s') ]$$
* 其中 $r$ 为当前状态从 $s$ 转移到 $s'$ 当中获取的奖励
* $v_{\pi}(s')$ 则是下一个状态的状态值期望（下一个状态$s'$的Q值加权平均期望）
* $p(s', r|s, a)$ 则是动态环境下的状态转移概率
* 计算： 计算$Q(s, a)$ 的值，是所有可能产生的“后续结果”的期望。每个后续结果的价值是 立即奖励$r$加上 未来回报的折现值 $\gamma v_π(s')$。这个期望是根据环境的转移概率 $p$来计算的。

> 通俗理解： “你选择某条特定道路（动作a）的前景如何（Q值）”，取决于“这条路能立刻带给你的回报（奖励r）”加上“这条路最终会把你带到的目的地的价值（下一状态的V值）”，并考虑到路况的不确定性（环境转移概率p）

| 特性       | **状态值函数 V(s)**     | **动作值函数 Q(s, a)**           |
| :------- | :----------------- | :-------------------------- |
| **评估对象** | 状态 (State)         | 状态-动作对 (State-Action Pair)  |
| **回答问题** | “处于这个状态有多好？”       | “执行这个动作有多好？”                |
| **依赖于**  | **策略 `π`**         | **策略 `π`** 和 **环境动态 `p`**   |
| **建模关系** | 是 `Q(s,a)` 在动作上的期望 | 是 `r + γV(s')` 在下一状态和奖励上的期望 |
|          |                    |                             |

## QA : 关于动作值函数当中的 $Q(s, a)$当中的$\gamma$是什么参数？
好的，这是一个非常核心的问题。图片中的 `γ` (gamma) 参数是强化学习中一个至关重要的概念。

**`γ` (gamma) 参数是折扣因子（Discount Factor）。**

它的作用是在计算一个动作的长期价值时，对未来的奖励进行“打折”。它是一个介于 0 和 1 之间的数字 (`0 ≤ γ ≤ 1`)。

### Gamma (γ) 存在的意义是什么？

`γ` 的存在主要有以下三个核心意义：

#### 1. 调节对“即时奖励”与“未来奖励”的重视程度

`γ` 控制了智能体（Agent）的“远见”程度。

*   **如果 `γ` 接近 0 (例如 `γ=0.1`)：**
    智能体将变得非常 **“短视” (Myopic)**。它几乎只关心能够立即获得的奖励 `r`，而不太在乎未来的奖励 `v(s')`。在这种设置下，智能体可能会选择一个能带来小但确定的即时好处的动作，即使这会导致未来进入一个很糟糕的状态。
    *   **例子：** 就像一个人选择马上吃掉一颗糖（即时奖励），而不愿意花时间去种一棵未来能结出很多果实的果树。

*   **如果 `γ` 接近 1 (例如 `γ=0.99`)：**
    智能体将变得非常有 **“远见” (Far-sighted)**。它认为未来的奖励和眼前的奖励几乎同等重要。它愿意为了获得更大的长期回报而牺牲掉一些眼前的利益。
    *   **例子：** 就像一个学生现在努力学习（牺牲了眼前的娱乐时间），为了未来能找到一份好工作（获得巨大的未来奖励）。

#### 2. 确保数学计算的收敛性

这是`γ`存在的一个非常重要的技术原因。

一个状态或动作的价值，理论上是未来所有奖励的加权和：
$\text{Total Reward} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots$

如果一个任务是**持续性的（没有终点）**，并且智能体可以一直获得正向奖励，那么当 `γ = 1` 时，这个总奖励的和可能会变成无穷大 (`∞`)。一个无穷大的价值是无法进行比较和优化的，算法也无法收敛。

通过设置 `γ < 1`，我们保证了即使在无限时间的任务中，这个奖励的级数和也是一个**有限的数**，从而保证了价值函数有界，算法（如价值迭代）可以稳定地收敛到一个解。

#### 3. 模拟未来的不确定性

未来的奖励本身就具有不确定性。100步之后才能获得的奖励，远不如马上就能拿到手的奖励来得可靠。

`γ` 可以被看作是 **“任务在下一步能继续进行的概率”**。`γ^k` 就可以被理解为任务在 `k` 步之后仍然在继续的概率。因此，对未来的奖励进行折扣，也隐式地考虑了模型的不完美、环境的随机性等因素，即越遥远的未来，不确定性越大，其价值也应该被相应地打折扣。

### 总结

`γ` (折扣因子) 是一个核心超参数，它：

1.  **定义了智能体的目标：** 是更看重眼前利益还是长期回报。
2.  **保证了算法的稳定性：** 确保了价值函数有界，使算法能够收敛。
3.  **体现了对未来不确定性的考量：** 越遥远的奖励，其价值折扣越大。

在实践中，如何设定 `γ` 的值，取决于具体任务的特性。对于需要长期规划、回报周期长的任务（如下棋），`γ` 通常设置得较高（如0.99）。对于反应更即时、不太需要长远规划的任务，`γ` 可以设置得低一些。