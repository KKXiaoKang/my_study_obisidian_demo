好的！下面我用一个简单的例子来演示Q-Learning的Q表更新过程。  
我们假设有两个状态 \( S_1, S_2 \)，两个动作 \( A_1, A_2 \)。  
初始Q表全为0，学习率 \(\alpha = 0.5\)，折扣因子 \(\gamma = 0.9\)。

### 状态和动作定义

- 状态：\( S_1, S_2 \)
- 动作：\( A_1, A_2 \)
- 所有S-A对：\((S_1, A_1), (S_1, A_2), (S_2, A_1), (S_2, A_2)\)

---

### 假设的环境反馈

| 步数  | 当前状态    | 动作      | 奖励 \(r\) | 下一个状态   |
| --- | ------- | ------- | -------- | ------- |
| 1   | \(S_1\) | \(A_1\) | 1        | \(S_2\) |
| 2   | \(S_2\) | \(A_2\) | 2        | \(S_1\) |
| 3   | \(S_1\) | \(A_2\) | 0        | \(S_2\) |
| 4   | \(S_2\) | \(A_1\) | 1        | \(S_2\) |
| 5   | \(S_2\) | \(A_2\) | 2        | \(S_1\) |
| 6   | \(S_1\) | \(A_1\) | 1        | \(S_2\) |
| 7   | \(S_2\) | \(A_2\) | 2        | \(S_1\) |
| 8   | \(S_1\) | \(A_2\) | 0        | \(S_2\) |
| 9   | \(S_2\) | \(A_1\) | 1        | \(S_2\) |
| 10  | \(S_2\) | \(A_2\) | 2        | \(S_1\) |

---

### Q表初始化

| 状态  | 动作  | 初始Q值 |
|-------|-------|---------|
| \(S_1\) | \(A_1\) | 0       |
| \(S_1\) | \(A_2\) | 0       |
| \(S_2\) | \(A_1\) | 0       |
| \(S_2\) | \(A_2\) | 0       |

---

### 每一步的Q值更新过程

Q-Learning更新公式：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
$$

---

#### 步骤推导表

| 步数  | \(s_t\) | \(a_t\) | \(r_t\) | \(s_{t+1}\) | Q表更新前                   | Q表更新后                   | 计算过程                                                     |
| --- | ------- | ------- | ------- | ----------- | ----------------------- | ----------------------- | -------------------------------------------------------- |
| 1   | \(S_1\) | \(A_1\) | 1       | \(S_2\)     | \(Q(S_1,A_1)=0\)        | \(Q(S_1,A_1)=0.5\)      | \(0+0.5[1+0.9\times0-0]=0.5\)                            |
| 2   | \(S_2\) | \(A_2\) | 2       | \(S_1\)     | \(Q(S_2,A_2)=0\)        | \(Q(S_2,A_2)=1.25\)     | \(0+0.5[2+0.9\times0.5-0]=1.25\)                         |
| 3   | \(S_1\) | \(A_2\) | 0       | \(S_2\)     | \(Q(S_1,A_2)=0\)        | \(Q(S_1,A_2)=0.28125\)  | \(0+0.5[0+0.9\times1.25-0]=0.5625\)（但Q(S_2,A_2)已更新为1.25） |
| 4   | \(S_2\) | \(A_1\) | 1       | \(S_2\)     | \(Q(S_2,A_1)=0\)        | \(Q(S_2,A_1)=0.5625\)   | \(0+0.5[1+0.9\times1.25-0]=0.5625\)                      |
| 5   | \(S_2\) | \(A_2\) | 2       | \(S_1\)     | \(Q(S_2,A_2)=1.25\)     | \(Q(S_2,A_2)=1.640625\) | \(1.25+0.5[2+0.9\times0.5-1.25]=1.640625\)               |
| 6   | \(S_1\) | \(A_1\) | 1       | \(S_2\)     | \(Q(S_1,A_1)=0.5\)      | \(Q(S_1,A_1)=1.003125\) | \(0.5+0.5[1+0.9\times1.640625-0.5]=1.003125\)            |
| 7   | \(S_2\) | \(A_2\) | 2       | \(S_1\)     | \(Q(S_2,A_2)=1.640625\) | \(Q(S_2,A_2)=2.026406\) | \(1.640625+0.5[2+0.9\times1.003125-1.640625]=2.026406\)  |
| 8   | \(S_1\) | \(A_2\) | 0       | \(S_2\)     | \(Q(S_1,A_2)=0.28125\)  | \(Q(S_1,A_2)=0.913883\) | \(0.28125+0.5[0+0.9\times2.026406-0.28125]=0.913883\)    |
| 9   | \(S_2\) | \(A_1\) | 1       | \(S_2\)     | \(Q(S_2,A_1)=0.5625\)   | \(Q(S_2,A_1)=1.412883\) | \(0.5625+0.5[1+0.9\times2.026406-0.5625]=1.412883\)      |
| 10  | \(S_2\) | \(A_2\) | 2       | \(S_1\)     | \(Q(S_2,A_2)=2.026406\) | \(Q(S_2,A_2)=2.474883\) | \(2.026406+0.5[2+0.9\times1.003125-2.026406]=2.474883\)  |

---

### 10步后Q表

| 状态  | 动作  | Q值（10步后） |
|-------|-------|--------------|
| \(S_1\) | \(A_1\) | 1.003125     |
| \(S_1\) | \(A_2\) | 0.913883     |
| \(S_2\) | \(A_1\) | 1.412883     |
| \(S_2\) | \(A_2\) | 2.474883     |

---

### 总结

- Q值从0开始，随着每次与环境的交互和奖励反馈，逐步逼近真实的期望回报。
- 每个S-A对都能被不断优化，最终收敛到最优Q值。

如需更详细的推导或代码实现，欢迎继续提问！