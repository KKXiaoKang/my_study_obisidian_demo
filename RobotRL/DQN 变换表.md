# DQN训练流程示例

下面我将通过一个具体数值示例，展示DQN的训练流程、数据流和反向传播过程。

## 1. 初始设置

假设我们有：
- 状态空间：2维向量表示位置 (x, y)
- 动作空间：4个离散动作 (上、下、左、右)
- 折扣因子 γ = 0.9
- 学习率 α = 0.01

## 2. 网络结构

- **当前网络 Q(s,a;θ)**：参数θ
- **目标网络 Q(s,a;θ⁻)**：参数θ⁻（从当前网络复制）

## 3. 数据流示例

假设从经验回放池中采样了一个批次（batch size=1）：
- 当前状态 s = (2, 3)
- 执行动作 a = 右
- 获得奖励 r = 1
- 下一状态 s' = (3, 3)

## 4. 前向传播计算

### 当前网络输出
假设当前网络对状态s=(2,3)的Q值预测为：
- Q(s,上;θ) = 0.5
- Q(s,下;θ) = 0.3
- Q(s,左;θ) = 0.2
- **Q(s,右;θ) = 0.6** (我们选择的动作)

### 目标网络输出
目标网络对下一状态s'=(3,3)的Q值预测为：
- Q(s',上;θ⁻) = 0.7
- Q(s',下;θ⁻) = 0.4
- Q(s',左;θ⁻) = 0.8
- Q(s',右;θ⁻) = 0.5

### 计算目标值
根据图中公式，目标值为：
```
target = r + γ * max_a' Q(s',a';θ⁻)
       = 1 + 0.9 * max(0.7, 0.4, 0.8, 0.5)
       = 1 + 0.9 * 0.8
       = 1 + 0.72
       = 1.72
```

### 计算损失函数
根据图中公式，MSE损失为：
```
L(θ) = (target - Q(s,a;θ))²
     = (1.72 - 0.6)²
     = (1.12)²
     = 1.2544
```

## 5. 反向传播过程

假设当前网络是一个简单的神经网络，最后一层权重为w，偏置为b，使得Q(s,右;θ) = w·h + b，其中h是上一层的输出。

### 第一次反向传播

计算损失函数对Q值的梯度：
```
∂L/∂Q(s,a;θ) = -2(target - Q(s,a;θ))
              = -2(1.72 - 0.6)
              = -2(1.12)
              = -2.24
```

计算Q值对w的梯度（假设h=1.5）：
```
∂Q(s,a;θ)/∂w = h = 1.5
```

计算损失函数对w的梯度：
```
∂L/∂w = ∂L/∂Q(s,a;θ) * ∂Q(s,a;θ)/∂w
       = -2.24 * 1.5
       = -3.36
```

更新w：
```
w_new = w - α * ∂L/∂w
      = w - 0.01 * (-3.36)
      = w + 0.0336
```

### 第二次反向传播

假设更新后，当前网络对状态s的Q值变为：
- Q(s,上;θ) = 0.52
- Q(s,下;θ) = 0.32
- Q(s,左;θ) = 0.22
- **Q(s,右;θ) = 0.65**

重新计算损失：
```
L(θ) = (1.72 - 0.65)²
     = (1.07)²
     = 1.1449
```

计算梯度：
```
∂L/∂Q(s,a;θ) = -2(1.72 - 0.65) = -2.14
∂L/∂w = -2.14 * 1.5 = -3.21
```

更新w：
```
w_new = w - 0.01 * (-3.21) = w + 0.0321
```

### 第三次反向传播

假设更新后，当前网络对状态s的Q值变为：
- Q(s,上;θ) = 0.54
- Q(s,下;θ) = 0.34
- Q(s,左;θ) = 0.24
- **Q(s,右;θ) = 0.70**

重新计算损失：
```
L(θ) = (1.72 - 0.70)²
     = (1.02)²
     = 1.0404
```

计算梯度：
```
∂L/∂Q(s,a;θ) = -2(1.72 - 0.70) = -2.04
∂L/∂w = -2.04 * 1.5 = -3.06
```

更新w：
```
w_new = w - 0.01 * (-3.06) = w + 0.0306
```

## 6. 训练进展总结

| 迭代次数 | Q(s,右;θ) | 目标值 | 损失 | 梯度 ∂L/∂w | 权重更新 |
|---------|-----------|-------|------|-----------|---------|
| 初始    | 0.60      | 1.72  | 1.2544 | -3.36    | +0.0336 |
| 第1次   | 0.65      | 1.72  | 1.1449 | -3.21    | +0.0321 |
| 第2次   | 0.70      | 1.72  | 1.0404 | -3.06    | +0.0306 |
| 第3次   | 0.73      | 1.72  | 0.9801 | -2.97    | +0.0297 |

可以看到，随着训练的进行，Q值逐渐接近目标值1.72，损失函数值逐渐减小。

## 7. 完整DQN训练流程

1. **初始化**：随机初始化当前网络参数θ，复制到目标网络θ⁻
2. **收集经验**：Agent与环境交互，收集(s,a,r,s')四元组存入经验池
3. **采样训练**：从经验池随机采样batch数据
4. **前向传播**：计算当前Q值和目标Q值
5. **计算损失**：使用图中MSE公式计算损失
6. **反向传播**：计算梯度并更新当前网络参数θ
7. **更新目标网络**：每C步复制当前网络参数到目标网络
8. **重复步骤2-7**直到收敛

这个过程通过固定目标网络参数，使训练更加稳定，避免了Q值的不断变化导致的训练不稳定问题。