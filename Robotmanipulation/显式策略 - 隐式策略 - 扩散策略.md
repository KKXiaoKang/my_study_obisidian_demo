![[Pasted image 20250625145928.png]]
### (c)  扩散策略（Diffusion Policy）
* 算法核心流程
	* 输入$\mathcal{O}$ （环境状态）
	* 初始化动作样本$a \sim \epsilon_\theta(\mathcal{O}, a)$ 
	* 然后再K步内，逐步沿着梯度逐步沿着梯度$\nabla E(o, a)$修正动作
	* 最终得到动作$a^v$
	* 右图显示了一个经典的**梯度场** Gradient Field，箭头表示动作如何被一步步调整以降低能量
### (a) 显式策略（Explicit Policy）
* 输入$\mathcal{O}$ （环境状态）
* 模型$\mathcal{F}_{\theta}(\mathcal{O})$ 显式输出动作$a$ 
* 动作 $a$ 包含如下
	* `Action Representation` : 动作表示, 连续的标量动作`Scalar(Regression)`（回归）
	* `Mixture of Gaussians` : 混合高斯分布模型
	* `Categorical` : 离散分布（分类任务）
* 总结
	- 这种方法直接建模 $\pi_{\theta}(a | o)$ 
	- 是`行为克隆（BC）`[[传统BC方法一览]]、`PPO`、`SAC` 等常规 RL 的策略模型方式
	- 优点：简单、推理快
	- 缺点：难以建模复杂动作空间，容易出错积累

### (b) 隐式策略（Implicit Policy）
* 模型不显式输出$a^v$ ，而是通过**优化一个能量函数** ${E}(\mathcal{O}, \mathcal{a})$ 来找最优动作
* 黑色函数$arg \min_{a} E(\mathcal{O}, \mathcal{a})$ : 动作是通过在动作空间中搜索最低能量的点
* 能量图可视化：动作分布对应能量低的区域（颜色浅）
* 特点：
	- 这种策略是**能量导向的（Energy-based Policy）**
	- 不提供明确的分布形式（没有 π(a∣o)\pi(a|o)π(a∣o)）
	- 是一种典型的**隐式策略建模**，如：
	    - GAIL（间接学习）
	    - Energy-based models（EBM）
	    - IRL（Inverse RL）
* 优缺点：
	- 优点：表达力强，可用于复杂高维动作空间
	- 缺点：推理过程慢（需要优化/采样），训练复杂

| 策略类型 | 输出动作方式          | 是否显式建模 π(a∣o)\pi(a | 推理方式   | 优缺点              |
| ---- | --------------- | ------------------ | ------ | ---------------- |
| 显式策略 | 直接输出动作/分布       | ✅ 是                | 一次前向   | 简单高效，但建模能力有限     |
| 隐式策略 | 优化能量函数求最优动作     | ❌ 否                | 最优化/采样 | 表达力强，但慢，训练不稳定    |
| 扩散策略 | 多步 denoise 输出动作 | ✅/❌ 视建模形式而定        | 多步反向采样 | 多模态建模强，但推理慢、需GPU |